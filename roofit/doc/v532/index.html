<br/> 
<hr/> 
<a name="roofit"></a> 
<h3>RooFit Package</h3>

    RooFit 3.50 has undergone a substantial amount of core engineering to improve computational efficiency
    and improve algorithmic likelihood optimizations. The expected increases in execution speed range from
    roughly 20% (for problems that were already implemented in a close-to optimal form) to more than 2000%
    for certain type of problems. Below is a summary of the changes made. All of these changes are
    transparent to end-use cases 
 
    <ul>
    <li><p> New implementation of RooFit data types. The implentation of data stored in RooDataSet and RooDataHist
         was historically handled by ROOT TTrees (though class RooTreeDataStore). The default storage type
         has now been changed to class RooVectorDataStore which stores the information in STL arrays. Existing
         datasets based on trees can be read in transparently, and are converted to vector form in the 
         persistent-to-transient conversion (the datafile is not modified in this operation)
</p><p> 
         The vector store has two important advantages: 1) faster data access (raw data access times are 70 times 
         faster than for TTrees), 2) ability to rewrite columns on the fly. The first advantage is important
         for the existing constant-term precalculation optimization in roofit likelihoods as these are now
         also stored in vectors rather than trees. The faster access speed of vectors make that the constant
         term optimization inside likelihoods results in a larger speed increase. This is particulatly noticable in pdfs with
         many constant expressions from pdfs that were moderately fast to begin with (e.g. RooHistPdf).
         The second advantages allows new types of algorithmic likelihood optimization in RooFit detailed below.
</p></li>
     <li><p>New algorithmic optimization in the caching of pdfs. So far - in the likelihood - two classes of
         objects are identified: those that change with every event (i.e. the pdf) and those that change
         only with the parameters (typically pdf normalization integrals). Pdfs are always recalculated
         for every event, whereas integrals are only evaluated when needed. The exception to the first type are pdfs
         that only depend on constant parameters (or no parameters) - these are identified at the beginning, and precalculated once 
         to avoid recalculating an expression with the same outcome for every iteration of the likelihood calculation
</p><p>
         For composite pdfs a further optimization has been included: for a M(x,a,b) = f*F(x,a)+(1-f)G(x,b) 
         it is e.g. not needed to recalculate G(x,b) if only parameter a has changed w.r.t to the previous likelihood
         calculation. This optimization is now implemented by extending the value caching orignally designed
         for constant terms to be usable for non-constant terms, with a check executed at the beginning of each
         likelihood evaluation if selected columns need to be updated because parameters have changed. The speed gain
         of this optimization depends much on the structure of the pdf: in models with many free parameters most of the
         likelihood evaluations are executed when MINUIT calculates numerical likelihood derivatives which vary 
         one parameter at a time and the speedup is potentially larger. In models with few free parameters the
         effect will be smaller.
</p><p>
         The new per-component caching strategy is enabled by default for all pdfs that are a component of
         a RooAddPdf or a RooRealSumPdf, unless that component is a RooProdPdf or a RooProduct, in that
         case the components of the product are cached instead of the product itself. You can disable this
         new optimization by adding Optimize(1) to the RooAbsPdf::fitTo() command line (0 = no caching,
         1 = cache constant terms only, 2 = cache also variable terms according to above mentioned strategy (DEFAULT))
</p><p>
         It is also possible to tune this 'cache-and-track' optimization to perform a more fine-grained caching
         of components than Optimize(2) implements: to do so, call arg->setAttribute("CacheAndTrack") on each
         pdf component that you'd like to be cache-and-tracked individually.
</p></li>
     <li>New pdf/data attach mechanism in likelihood objects (RooAbsOptTestStatistic). The new mechanism only
         reattaches the dataset branch buffers and not the RooRealVars representing the data. This new designs
         allows for a much faster RooAbsTestStatistic::setData() implementation, which changes the dataset in
         an existing likelihood object. This will speed up RooStats tools based on 'simple' likelihood models
         substantially.
</li>
     <li>Automatic detections of 'binned' pdfs and automatic generation of binned data in generate(). RooFit will
         now automatically generate binned pdf shapes. Binned pdfs shapes are fundamentally RooHistPdf and RooHistFuncs
         (with interpolation order set to zero). Products and sums of exclusively binned shapes are also recognized
         as binned shapes. For such binned shapes generate() will now by default follow the 'binned' strategy 
         -- that is, take the expectation value in each bin and sample a Poisson distribution from that -- rather
         than follow the unbinned strategy. The rationale is that such datasets result in much faster likelihood
         calculations (for nbin smaller than nevent). The optimization is also exact: the likelihood of a binned 
         data using a binned pdf is identical to that of an unbinned dataset with a binned pdf. Nevertheless you can 
         switch off this feature by passing AutoBinned(false) to RooAbsPdf::generate().
</li>
     <li>Mixed binned/unbinned generation from simultaneous pdf. For a RooSimultaneous consisting of exclusively
         extended terms it is now possible to generate a mixed binned/unbinned datasets. Components defined
         by a binned pdf at the top level are automatically generated binned (unless AutoBinned(false) is set)
         but it is also possible to generate other component pdfs forcibly binned by adding GenBinned(tagname)
         to generate(). In that case all component pdfs labeled with pdf->setAttribute(tagname) will be generated
         binned. To generate all component binned, the shorthand method AllBinned() can be used. All binned
         datasets made by generate are repesented as weighted unbinned datasets (of type RooDataSet) rather
         than binned datasets of type RooDataHist so that mixed binned/unbinned data is always represented
         through a uniform interface.
</li>
      <li>Fix in the optimization strategy of likelihoods constructed from simultaneous pdf. In the parameter
          dependency analysis of the components of a simultaneous pdfs parameters originating from 'irrelevant'
          constraint terms (i.e. those that don't relate to any of the parameters of that likelihood component) were
          not ignored, which could result in a substantial loss of computational efficiency as likelihood
          terms were erroneously recalculated even if no relevant parameters was changed.
</li>
      <li>General performance tuning of RooFit to reduce computational overhead. Extensive profiling of
          CPU times in call graphas and and analysis heap memory use have been performed and many small 
          changes have been made to make the code more efficient and use less memory.
</li>
      </ul>

<a name="roostats"></a> 
<h3>RooStats Package</h3>

<h4>AsymptoticCalculator</h4>
<ul>
<li>New Class for doing an hypothesis tests using the asymptotic likelihood formulae, described in the paper from
   G. Cowan, K. Cranmer, E. Gross and O. Vitells, <i>Asymptotic formulae for likelihood- based tests of new physics</i>,
   Eur. Phys. J., C71 (1), 2011.</li>
   <li> The class computes the p-value for the null and also for the alternate using the Asimov data set. In this
   differs form the ProfileLikelihoodCalculator which computes only the p-values for the null hypothesis.</li>
   <li>The Asimov data set is generated with the utility function <tt>AsymptoticCalculator::MakeAsimovData</tt> and then
   it is used to evaluate the likelihood. 
   </li>
<li>This class implements the HypoTestCalculatorGeneric interface and can be used as an alternative Hypotesis test
   calculator in the HypoTestInverter class. It can then plugged in the HypoTestInverter for computing asymptotic CLs and CLs+b
   limits. In this way the limits will be computed by just performing a fit for each test parameter value and without
   generating any toys.  
</li>
<li>The class can be used via the <tt>StandardHypothesisTest.C</tt> tutorial passing a  value of <tt>2</tt> for the
calculator type. </li>
</ul>

<h4>RooStats Utils</h4>
<ul>
  <li>Add a utility function (from G. Petrucciani), <tt>RooStats::MakeNuisancePdf</tt>, which given a model configuration (or the global  pdf and the
  observables), factorizes from the model pdf the constraint probability density functions for the nuisance parameters
  and builds a global nuisance pdf. This function can then be used in the HybridCalculator or in the BayesianCalculator
  with the option "TOYMC".
  </li>
</ul>

<h4>HypotestInverter and HypoTestInverterResult</h4>
<ul>
<li> Several improvements and bug fixes in merging results and in computing the observed and expected limits.</li>
<li> Provide support now for using the AsympoticCalculator</li>
</ul>

<h4>MCMCCalculator</h4>
<ul>
  <li>Add now possibility to store in the chain only the parameter of interested via the method <tt> MCMCCalculator::SetChainParameters</tt>. This saves memory in case of models with a
  large number of nuisance parameters. </li>
</ul>

<h4>Test Statistics classes</h4>
<ul>
  <li>Make a more robust evaluation of the ProfileLikelihoodTestStat. Use RooMinimizer and give possibility to use
  different minimizer, via <tt>ProfileLikelihoodTestStat::SetMinimizer</tt>. The print level of minimization can also be
  controlled via <tt>ProfileLikelihoodTestStat::SetPrintLevel</tt>. Activate also the RooFit cache optimizations when
  evaluating the NLL </li>
  <li>The same optimizations are applied also to the <tt>RatioOfProfilesLikelihood</tt> test statistic</li>
  <li>Fix a bug in reusing the NLL object in the SimpleLikelihoodCalculator. This makes now the evaluation of this test
  statistics much faster. </li>
 </ul>
