\begin{optiontableAuto}
                   NTrees  &  \mc{1}{c}{--}  &              800  &  \mc{1}{l}{--}  &  Number of trees in the forest \\
                 MaxDepth  &  \mc{1}{c}{--}  &                3  &  \mc{1}{l}{--}  &  Max depth of the decision tree allowed \\
              MinNodeSize  &  \mc{1}{c}{--}  &               5\%  &  \mc{1}{l}{--}  &  Minimum percentage of training events required in a leaf node (default: Classification: 5\%, Regression: 0.2\%) \\
                    nCuts  &  \mc{1}{c}{--}  &               20  &  \mc{1}{l}{--}  &  Number of grid points in variable range used in finding optimal cut in node splitting \\
                BoostType  &  \mc{1}{c}{--}  &         AdaBoost  &  AdaBoost, RealAdaBoost, AdaCost, Bagging, AdaBoostR2, Grad  &  Boosting type for the trees in the forest (note: AdaCost is still experimental) \\
           AdaBoostR2Loss  &  \mc{1}{c}{--}  &        Quadratic  &  Linear, Quadratic, Exponential  &  Type of Loss function in AdaBoostR2 \\
            UseBaggedGrad  &  \mc{1}{c}{--}  &            False  &  \mc{1}{l}{--}  &  Use only a random subsample of all events for growing the trees in each iteration. (Only valid for GradBoost) \\
                Shrinkage  &  \mc{1}{c}{--}  &                1  &  \mc{1}{l}{--}  &  Learning rate for GradBoost algorithm \\
             AdaBoostBeta  &  \mc{1}{c}{--}  &              0.5  &  \mc{1}{l}{--}  &  Learning rate  for AdaBoost algorithm \\
       UseRandomisedTrees  &  \mc{1}{c}{--}  &            False  &  \mc{1}{l}{--}  &  Determine at each node splitting the cut variable only as the best out of a random subset of variables (like in RandomForests) \\
                 UseNvars  &  \mc{1}{c}{--}  &                2  &  \mc{1}{l}{--}  &  Size of the subset of variables used with RandomisedTree option \\
          UsePoissonNvars  &  \mc{1}{c}{--}  &             True  &  \mc{1}{l}{--}  &  Interpret UseNvars not as fixed number but as mean of a Possion distribution in each split with RandomisedTree option \\
     BaggedSampleFraction  &  \mc{1}{c}{--}  &              0.6  &  \mc{1}{l}{--}  &  Relative size of bagged event sample to original size of the data sample (used whenever bagging is used (i.e. UseBaggedGrad, Bagging,) \\
             UseYesNoLeaf  &  \mc{1}{c}{--}  &             True  &  \mc{1}{l}{--}  &  Use Sig or Bkg categories, or the purity=S/(S+B) as classification of the leaf node -$>$ Real-AdaBoost \\
       NegWeightTreatment  &  \mc{1}{c}{--}  &  {\tiny InverseBoostNegWeights}  & {\tiny InverseBoostNegWeights, IgnoreNegWeightsInTraining, PairNegWeightsGlobal}  &  How to treat events with negative weights in the BDT training (particular the boosting) : IgnoreInTraining;  Boost With inverse boostweight; Pair events with negative and positive weights in traning sample and *annihilate* them (experimental!) 
\end{optiontableAuto}
